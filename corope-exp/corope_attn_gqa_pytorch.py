import torch
import torch.nn.functional as F

def attention_pytorch_block_shared(q, k, v, causal, sm_scale, theta, block_m=64):
    """
    PyTorch implementation of Co-RoPE attention with Block-Shared Leader strategy.
    
    This function uses PyTorch Autograd for backward pass, which is the 
    Ground Truth for verifying the Triton kernel.
    
    Args:
        q: (BATCH, H, N_CTX, HEAD_DIM)
        k: (BATCH, H, N_CTX, HEAD_DIM)
        v: (BATCH, H, N_CTX, HEAD_DIM)
        causal: bool
        sm_scale: float
        theta: float
        block_m: int, MUST match the BLOCK_M used in Triton kernel
    """
    if not causal:
        raise ValueError("Current verification only supports causal=True")

    B, n_heads_q, N_CTX, HEAD_DIM = q.shape
    n_heads_kv = k.shape[1]
    device = q.device
    
    # ç¡®ä¿ N_CTX èƒ½è¢« block_m æ•´é™¤ï¼Œæ–¹ä¾¿ reshape/repeat
    # å¦‚æœ Triton Kernel æ”¯æŒéæ•´é™¤ï¼Œè¿™é‡Œéœ€è¦åš padding å¤„ç†
    if N_CTX % block_m != 0:
        print(f"Warning: N_CTX({N_CTX}) is not divisible by block_m({block_m}). "
              "Padding for verification logic.")
        # ç®€å•å¤„ç†ï¼šæˆªæ–­æˆ–æŠ¥é”™ï¼Œè¿™é‡Œä¸ºäº†æ¼”ç¤ºé€»è¾‘å‡è®¾æ•´é™¤
        pass

    # Compute RoPE frequencies
    inv_freq = 1.0 / (theta ** (torch.arange(0, HEAD_DIM, 2, device=device).float() / HEAD_DIM))

    # Handle GQA (Group Query Attention)
    group_size = n_heads_q // n_heads_kv
    
    # Expand K, V for broadcasting interactions
    # shape: (B, H_Q, N, D)
    if group_size > 1:
        # è¿™é‡Œçš„ expand é€»è¾‘æ˜¯ä¸ºäº†è®© Q å’Œ K èƒ½ç‚¹ç§¯
        k_expanded = k.view(B, n_heads_kv, 1, N_CTX, HEAD_DIM).expand(
            B, n_heads_kv, group_size, N_CTX, HEAD_DIM
        ).reshape(B, n_heads_q, N_CTX, HEAD_DIM)
        v_expanded = v.view(B, n_heads_kv, 1, N_CTX, HEAD_DIM).expand(
            B, n_heads_kv, group_size, N_CTX, HEAD_DIM
        ).reshape(B, n_heads_q, N_CTX, HEAD_DIM)
    else:
        k_expanded = k
        v_expanded = v

    # ============================================================
    # Co-RoPE Block-Shared Logic
    # ============================================================
    
    # 1. æå– Leaders (Block Shared & GQA Shared)
    # ç‰©ç†å«ä¹‰ï¼šæ¯ä¸ª Block (block_m) åªæœ‰ 1 ä¸ª Leaderï¼Œæ¯ä¸ª GQA Group åªæœ‰ 1 ä¸ª Leader
    # Shape: [B, H_KV, N_BLOCKS, D]
    # æ³¨æ„ï¼šè¿™é‡Œå–çš„æ˜¯åŸå§‹ k (H_KV)ï¼Œæ‰€ä»¥ Q ä¹Ÿåªå– H_KV ä¸ª
    q_leaders = q[:, ::group_size, 0::block_m, :]
    
    N_BLOCKS = q_leaders.shape[2]
    
    # 2. è®¡ç®— Block Leader çš„é‡Œç¨‹ (Discovery Pass)
    # q_leaders: [B, H_KV, N_BLOCKS, D]
    # k:         [B, H_KV, N_CTX,    D]
    # raw_dot:   [B, H_KV, N_BLOCKS, N_CTX]
    raw_dot = torch.einsum('bhqd,bhkd->bhqk', q_leaders, k)
    
    # Sigmoid
    z_dist = torch.sigmoid(raw_dot * sm_scale)
    
    # Masking: Leader at block i can only accumulate mileage from k < block_start_i
    # (assuming the leader is the first token of the block)
    leader_indices = torch.arange(0, N_CTX, block_m, device=device) # [N_BLOCKS]
    k_indices = torch.arange(N_CTX, device=device)                  # [N_CTX]
    
    # mask: [1, 1, N_BLOCKS, N_CTX]
    # Leader i åªèƒ½çœ‹åˆ° K index <= Leader index çš„éƒ¨åˆ† (Causal)
    mileage_mask = leader_indices[:, None] >= k_indices[None, :]
    z_dist = torch.where(mileage_mask[None, None, :, :], z_dist, 0.0)

    # 3. è®¡ç®— Running Mileage å’Œ Total Mileage
    # a_running[..., n] è¡¨ç¤º Leader é‡åˆ° Key n æ—¶çš„ç´¯è®¡é‡Œç¨‹
    a_running = torch.cumsum(z_dist, dim=-1) # [B, H_KV, N_BLOCKS, N_CTX]
    
    # a_total[..., b] è¡¨ç¤º Leader b åœ¨ mask èŒƒå›´å†…çš„æ€»é‡Œç¨‹
    # ç”±äºæˆ‘ä»¬ mask äº†åç»­çš„ Kï¼Œå–æœ€åä¸€ä¸ªå€¼å³ä¸ºæ€»é‡Œç¨‹
    a_total = a_running[..., -1].unsqueeze(-1) # [B, H_KV, N_BLOCKS, 1]
    
    # 4. å¹¿æ’­ (Broadcast) å›å…¨åˆ†è¾¨ç‡
    # å°† N_BLOCKS ç»´åº¦æ‹‰ä¼¸å› N_CTX
    # [B, H_KV, N_CTX, N_CTX]
    a_running_expanded = a_running.repeat_interleave(block_m, dim=2)
    # [B, H_KV, N_CTX, 1]
    a_total_expanded = a_total.repeat_interleave(block_m, dim=2)
    
    # å¤„ç†æˆªæ–­é—®é¢˜ï¼šå¦‚æœ N_CTX ä¸æ˜¯ block_m çš„æ•´æ•°å€ï¼Œrepeat åä¼šå¤šå‡ºæ¥ï¼Œéœ€è¦åˆ‡ç‰‡
    if a_running_expanded.shape[2] > N_CTX:
        a_running_expanded = a_running_expanded[:, :, :N_CTX, :]
        a_total_expanded = a_total_expanded[:, :, :N_CTX, :]
        
    # GQA Broadcast: å¤åˆ¶ç»™ç»„å†…çš„æ‰€æœ‰ Heads
    # [B, H_Q, N_CTX, N_CTX]
    a_running_final = a_running_expanded.repeat_interleave(group_size, dim=1)
    # [B, H_Q, N_CTX, 1]
    a_total_final = a_total_expanded.repeat_interleave(group_size, dim=1)
    
    # 5. è®¡ç®— Delta A (Phase)
    # Co-RoPE Phase: (a_query_total - a_key_current)
    # æ³¨æ„ç»´åº¦å¹¿æ’­: [B, H, N, 1] - [B, H, N, N] -> [B, H, N, N]
    delta_a = a_total_final - a_running_final
    
    # ============================================================
    # Standard Attention with Rotated Score
    # ============================================================

    # Split Q, K into halves for RoPE pairs
    d_half = HEAD_DIM // 2
    q1, q2 = q[..., :d_half], q[..., d_half:]
    k1, k2 = k_expanded[..., :d_half], k_expanded[..., d_half:]
    
    # Pre-rotation dot products (EA, EB)
    # Dimensions: [B, H, N, N, D/2]
    # æ³¨æ„è¿™é‡Œæ˜¾å­˜å ç”¨å·¨å¤§ï¼ŒN_CTX è¾ƒå¤§æ—¶å¯èƒ½ä¼š OOMï¼Œä»…ä¾›å¯¹æ‹å°åºåˆ—ä½¿ç”¨
    E_A = q1.unsqueeze(3) * k1.unsqueeze(2) + q2.unsqueeze(3) * k2.unsqueeze(2)
    E_B = q2.unsqueeze(3) * k1.unsqueeze(2) - q1.unsqueeze(3) * k2.unsqueeze(2)
    
    # Calculate Phase
    # [B, H, N, N, D/2]
    phi = delta_a.unsqueeze(-1) * inv_freq.view(1, 1, 1, 1, -1)
    
    # Apply Rotation
    # score = E_A * cos(phi) - E_B * sin(phi)
    # Sum over head_dim halves
    attn_scores = (E_A * torch.cos(phi) - E_B * torch.sin(phi)).sum(dim=-1) * sm_scale
    
    # Standard Causal Masking (for Attention, distinct from mileage mask)
    mask = torch.triu(torch.ones(N_CTX, N_CTX, device=device, dtype=torch.bool), diagonal=1)
    attn_scores = attn_scores.masked_fill(mask[None, None, :, :], float('-inf'))
    
    # Softmax
    p = torch.softmax(attn_scores.float(), dim=-1).to(q.dtype)
    
    # Output projection
    output = torch.matmul(p, v_expanded)
    
    return output


# ==========================================
# éªŒè¯è„šæœ¬
# ==========================================
if __name__ == "__main__":
    torch.manual_seed(42)
    
    # è®¾ç½®å‚æ•°ï¼Œç¡®ä¿ N_CTX æ˜¯ BLOCK_M çš„å€æ•°
    BLOCK_M_VAL = 64
    B, H, N, D = 2, 4, 128, 64 
    sm_scale = D ** -0.5
    theta = 10000.0
    causal = True
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 # ä½¿ç”¨ fp16 æµ‹è¯•

    print(f"ğŸ”¬ Starting Block-Shared Co-RoPE Verification")
    print(f"   Config: B={B}, H={H}, N={N}, D={D}, BLOCK_M={BLOCK_M_VAL}")

    # å‡†å¤‡è¾“å…¥
    q = torch.randn(B, H, N, D, device=device, dtype=dtype, requires_grad=True)
    k = torch.randn(B, H, N, D, device=device, dtype=dtype, requires_grad=True)
    v = torch.randn(B, H, N, D, device=device, dtype=dtype, requires_grad=True)

    # -------------------------------------------------------
    # 1. è¿è¡Œ PyTorch Reference (Autograd Ground Truth)
    # -------------------------------------------------------
    ref_out = attention_pytorch_block_shared(
        q, k, v, 
        causal=causal, 
        sm_scale=sm_scale, 
        theta=theta, 
        block_m=BLOCK_M_VAL
    )

    # è®¡ç®— Reference Backward
    loss_ref = ref_out.sum()
    loss_ref.backward()
    grad_q_ref = q.grad.clone()
    grad_k_ref = k.grad.clone()
    grad_v_ref = v.grad.clone()

    print("âœ… PyTorch Reference computed.")

    # -------------------------------------------------------
    # 2. è¿è¡Œ Triton Kernel (æ­¤å¤„è°ƒç”¨ä½ çš„ Triton å‡½æ•°)
    # -------------------------------------------------------
    # æ¸…ç©ºæ¢¯åº¦
    q.grad = None
    k.grad = None
    v.grad = None
    
    print("ğŸš€ Running Triton Kernel...")
    try:
        # å‡è®¾ä½ çš„ Triton å°è£…å‡½æ•°å«åš attention
        # åŠ¡å¿…ç¡®ä¿ Triton å†…éƒ¨ä½¿ç”¨çš„ BLOCK_M ä¸ BLOCK_M_VAL ä¸€è‡´ (64)
        # å¹¶ä¸” Autotune å·²å…³é—­æˆ–è¢«é™åˆ¶ä¸ºä»…ä½¿ç”¨ BLOCK_M=64
        tri_out = attention(q, k, v, causal, sm_scale, warp_specialize=False)
        
        # Triton Backward
        loss_tri = tri_out.sum()
        loss_tri.backward()
        
        # -------------------------------------------------------
        # 3. ç»“æœå¯¹æ¯”
        # -------------------------------------------------------
        print("\nğŸ” Comparison Results:")
        
        # Forward å¯¹æ¯”
        fwd_diff = (ref_out - tri_out).abs().max().item()
        print(f"Forward Max Diff: {fwd_diff:.4e}")
        
        # Backward å¯¹æ¯”
        # æ³¨æ„ï¼šç”±äº float16 çš„ç´¯åŠ è¯¯å·®ï¼ŒTriton å’Œ PyTorch çš„å·®è·å¯èƒ½åœ¨ 1e-3 å·¦å³
        dq_diff = (grad_q_ref - q.grad).abs().max().item()
        dk_diff = (grad_k_ref - k.grad).abs().max().item()
        dv_diff = (grad_v_ref - v.grad).abs().max().item()
        
        print(f"dQ Max Diff:      {dq_diff:.4e}")
        print(f"dK Max Diff:      {dk_diff:.4e}")
        print(f"dV Max Diff:      {dv_diff:.4e}")
        
        # ç®€å•åˆ¤æ–­
        tol = 1e-2 if dtype == torch.float16 else 1e-4
        if fwd_diff < tol and dq_diff < tol:
            print("\nâœ¨ Match! The Triton kernel correctly implements Block-Shared Logic.")
        else:
            print("\nâš ï¸ Mismatch! Check leader selection logic or mask boundaries.")

    except NameError:
        print("\nâš ï¸ Triton function 'attention' not found. skipping triton run.")
    except Exception as e:
        print(f"\nâŒ Triton Run Failed: {e}")
        import traceback
        traceback.print_exc()