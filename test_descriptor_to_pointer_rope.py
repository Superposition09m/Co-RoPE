"""
æµ‹è¯•åœ¨ TensorDescriptor æ¶æ„ä¸‹èƒ½å¦è½¬æ¢ä¸ºæŒ‡é’ˆè¿›è¡Œ RoPE æ“ä½œ

ç›®æ ‡ï¼šéªŒè¯ä»¥ä¸‹æµç¨‹çš„å¯è¡Œæ€§
1. ä½¿ç”¨ TensorDescriptor ä½œä¸ºä¸»è¦æ•°æ®ç»“æ„
2. åœ¨éœ€è¦ RoPE æ—‹è½¬æ—¶ï¼Œä¸´æ—¶è½¬æ¢ä¸ºæŒ‡é’ˆ
3. ä½¿ç”¨åŒæŒ‡é’ˆåˆ†åˆ«åŠ è½½å‰åŠ/ååŠç»´åº¦
4. åº”ç”¨ RoPE æ—‹è½¬
5. ç»§ç»­ä½¿ç”¨ descriptor æˆ–æŒ‡é’ˆè¿›è¡Œåç»­è®¡ç®—

å‚è€ƒä»£ç ï¼š
- flash_attn_co_rope_gqa_triton.py (descriptor-based)
- flash_attn_rope_opt_triton.py (pointer-based with dual-pointer RoPE)
"""

import torch
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor

DEVICE = triton.runtime.driver.active.get_active_torch_device()

def is_cuda():
    return triton.runtime.driver.active.get_current_target().backend == "cuda"

def supports_host_descriptor():
    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9


@triton.jit
def _rope_with_descriptor_to_pointer(
    K_desc_or_ptr,  # è¾“å…¥ï¼šå¯ä»¥æ˜¯ descriptor æˆ– pointer
    K_out,  # è¾“å‡ºï¼šæ—‹è½¬åçš„ Kï¼ˆä½¿ç”¨æŒ‡é’ˆå†™å›ï¼‰
    freqs_cos_ptr, freqs_sin_ptr,
    N_CTX, HEAD_DIM: tl.constexpr,
    stride_k_seq, stride_k_dim,
    stride_freqs_seq, stride_freqs_dim,
    USE_DESCRIPTOR: tl.constexpr,
):
    """
    æµ‹è¯• kernelï¼šä» descriptor/pointer è¯»å– Kï¼Œåº”ç”¨ RoPEï¼Œå†™å›ç»“æœ
    
    å…³é”®æµ‹è¯•ç‚¹ï¼š
    1. å¦‚æœè¾“å…¥æ˜¯ descriptorï¼Œèƒ½å¦æå–å‡º base pointer
    2. ä½¿ç”¨åŒæŒ‡é’ˆåˆ†åˆ«åŠ è½½ k1, k2ï¼ˆå‰åŠ/ååŠç»´åº¦ï¼‰
    3. åº”ç”¨ RoPE æ—‹è½¬
    4. å†™å›ç»“æœ
    """
    pid = tl.program_id(0)
    
    half_dim: tl.constexpr = HEAD_DIM // 2
    offs_n = pid * 32 + tl.arange(0, 32)  # å‡è®¾ BLOCK_SIZE=32
    offs_d_first = tl.arange(0, half_dim)
    offs_d_second = offs_d_first + half_dim
    
    mask_k = (offs_n[:, None] < N_CTX)
    
    # ============================================
    # å…³é”®æµ‹è¯•ï¼šä» descriptor è½¬æ¢åˆ° pointer
    # ============================================
    if USE_DESCRIPTOR:
        # æ–¹æ¡ˆ1ï¼šå¦‚æœè¾“å…¥æ˜¯ TensorDescriptorï¼Œå°è¯•æå– base pointer
        # æ³¨æ„ï¼šTriton çš„ descriptor å¯èƒ½ä¸ç›´æ¥æ”¯æŒæå– base pointer
        # è¿™é‡Œæˆ‘ä»¬æµ‹è¯•æ˜¯å¦å¯ä»¥é€šè¿‡ä¼ å…¥åŸå§‹ pointer æ¥ç»•è¿‡
        K_ptr = K_desc_or_ptr
    else:
        # æ–¹æ¡ˆ2ï¼šç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ pointer
        K_ptr = K_desc_or_ptr
    
    # ============================================
    # åŒæŒ‡é’ˆåŠ è½½ K çš„å‰åŠå’ŒååŠç»´åº¦ï¼ˆå‚è€ƒ flash_attn_rope_opt_triton.pyï¼‰
    # ============================================
    k1_ptrs = K_ptr + offs_n[:, None] * stride_k_seq + offs_d_first[None, :] * stride_k_dim
    k2_ptrs = K_ptr + offs_n[:, None] * stride_k_seq + offs_d_second[None, :] * stride_k_dim
    
    mask_k_half = (offs_n[:, None] < N_CTX)
    k1 = tl.load(k1_ptrs, mask=mask_k_half, other=0.0)
    k2 = tl.load(k2_ptrs, mask=mask_k_half, other=0.0)
    
    # ============================================
    # åŠ è½½ RoPE é¢‘ç‡
    # ============================================
    freqs_cos_ptrs = freqs_cos_ptr + offs_n[:, None] * stride_freqs_seq + offs_d_first[None, :] * stride_freqs_dim
    freqs_sin_ptrs = freqs_sin_ptr + offs_n[:, None] * stride_freqs_seq + offs_d_first[None, :] * stride_freqs_dim
    
    cos_k = tl.load(freqs_cos_ptrs, mask=mask_k_half, other=1.0).to(tl.float32)
    sin_k = tl.load(freqs_sin_ptrs, mask=mask_k_half, other=0.0).to(tl.float32)
    
    # ============================================
    # åº”ç”¨ RoPE æ—‹è½¬ï¼ˆå‚è€ƒ flash_attn_rope_opt_triton.pyï¼‰
    # ============================================
    k1_rot = (k1.to(tl.float32) * cos_k - k2.to(tl.float32) * sin_k).to(tl.float16)
    k2_rot = (k2.to(tl.float32) * cos_k + k1.to(tl.float32) * sin_k).to(tl.float16)
    
    # ============================================
    # åŒæŒ‡é’ˆå†™å›ï¼ˆåˆ†åˆ«å†™å›å‰åŠå’ŒååŠç»´åº¦ï¼‰
    # ============================================
    k1_out_ptrs = K_out + offs_n[:, None] * stride_k_seq + offs_d_first[None, :] * stride_k_dim
    k2_out_ptrs = K_out + offs_n[:, None] * stride_k_seq + offs_d_second[None, :] * stride_k_dim
    
    tl.store(k1_out_ptrs, k1_rot, mask=mask_k_half)
    tl.store(k2_out_ptrs, k2_rot, mask=mask_k_half)


def test_descriptor_to_pointer_rope():
    """
    æµ‹è¯•ï¼šåœ¨ descriptor-based æ¶æ„ä¸­ä½¿ç”¨ pointer è¿›è¡Œ RoPE æ“ä½œ
    """
    print("="*60)
    print("æµ‹è¯•ï¼šTensorDescriptor -> Pointer -> RoPE -> Pointer å†™å›")
    print("="*60)
    
    # æµ‹è¯•å‚æ•°
    N_CTX = 128
    HEAD_DIM = 64
    BLOCK_SIZE = 32
    dtype = torch.float16
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    k = torch.randn((N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE)
    freqs_cos = torch.randn((N_CTX, HEAD_DIM // 2), dtype=torch.float16, device=DEVICE)
    freqs_sin = torch.randn((N_CTX, HEAD_DIM // 2), dtype=torch.float16, device=DEVICE)
    
    # è¾“å‡ºå¼ é‡
    k_out_pointer = torch.zeros_like(k)
    k_out_descriptor = torch.zeros_like(k)
    
    # Grid é…ç½®
    grid = (triton.cdiv(N_CTX, BLOCK_SIZE),)
    
    print(f"\nè¾“å…¥å½¢çŠ¶: K={k.shape}, freqs_cos={freqs_cos.shape}")
    print(f"Grid: {grid}, BLOCK_SIZE={BLOCK_SIZE}")
    
    # ============================================
    # æµ‹è¯•1ï¼šä½¿ç”¨çº¯æŒ‡é’ˆï¼ˆbaselineï¼‰
    # ============================================
    print("\n[æµ‹è¯•1] çº¯æŒ‡é’ˆæ–¹å¼ (baseline)")
    _rope_with_descriptor_to_pointer[grid](
        k,  # ç›´æ¥ä¼ å…¥ tensorï¼ˆTriton ä¼šè½¬æ¢ä¸º pointerï¼‰
        k_out_pointer,
        freqs_cos, freqs_sin,
        N_CTX, HEAD_DIM,
        k.stride(0), k.stride(1),
        freqs_cos.stride(0), freqs_cos.stride(1),
        USE_DESCRIPTOR=False,
    )
    
    print(f"âœ… çº¯æŒ‡é’ˆæ–¹å¼å®Œæˆ")
    print(f"   è¾“å‡ºèŒƒå›´: [{k_out_pointer.min().item():.4f}, {k_out_pointer.max().item():.4f}]")
    
    # ============================================
    # æµ‹è¯•2ï¼šå°è¯•ä½¿ç”¨ descriptorï¼ˆå¦‚æœæ”¯æŒï¼‰
    # ============================================
    if supports_host_descriptor():
        print("\n[æµ‹è¯•2] TensorDescriptor æ–¹å¼")
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä»ç„¶ä¼ å…¥åŸå§‹ pointerï¼Œå› ä¸ºåœ¨ kernel å†…éƒ¨
        # TensorDescriptor çš„ base pointer æå–å¯èƒ½ä¸ç›´æ¥æ”¯æŒ
        # å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¼šåœ¨å¤–å±‚ç®¡ç† descriptorï¼Œå†…å±‚ä½¿ç”¨ pointer
        _rope_with_descriptor_to_pointer[grid](
            k,  # ä¼ å…¥ç›¸åŒçš„ tensor
            k_out_descriptor,
            freqs_cos, freqs_sin,
            N_CTX, HEAD_DIM,
            k.stride(0), k.stride(1),
            freqs_cos.stride(0), freqs_cos.stride(1),
            USE_DESCRIPTOR=True,  # æ ‡è®°ä¸º descriptor æ¨¡å¼ï¼ˆè™½ç„¶å®ç°ç›¸åŒï¼‰
        )
        
        print(f"âœ… Descriptor æ–¹å¼å®Œæˆ")
        print(f"   è¾“å‡ºèŒƒå›´: [{k_out_descriptor.min().item():.4f}, {k_out_descriptor.max().item():.4f}]")
        
        # éªŒè¯ä¸¤ç§æ–¹å¼ç»“æœä¸€è‡´
        diff = (k_out_pointer - k_out_descriptor).abs().max().item()
        print(f"\nğŸ“Š ç»“æœå¯¹æ¯”:")
        print(f"   æœ€å¤§å·®å¼‚: {diff:.2e}")
        
        if diff < 1e-5:
            print(f"   âœ… ä¸¤ç§æ–¹å¼ç»“æœä¸€è‡´ï¼")
        else:
            print(f"   âš ï¸  ç»“æœå­˜åœ¨å·®å¼‚ï¼Œéœ€è¦æ£€æŸ¥")
    else:
        print("\n[è·³è¿‡æµ‹è¯•2] å½“å‰è®¾å¤‡ä¸æ”¯æŒ TensorDescriptor (éœ€è¦ Hopper+)")
    
    # ============================================
    # æµ‹è¯•3ï¼šPyTorch å‚è€ƒå®ç°éªŒè¯æ­£ç¡®æ€§
    # ============================================
    print("\n[æµ‹è¯•3] ä¸ PyTorch å‚è€ƒå®ç°å¯¹æ¯”")
    
    k_ref = k.clone()
    half_dim = HEAD_DIM // 2
    k1_ref = k_ref[:, :half_dim]
    k2_ref = k_ref[:, half_dim:]
    
    # æ‰©å±• freqs_cos/sin ä»¥åŒ¹é…å®Œæ•´ç»´åº¦ï¼ˆå®é™…åªç”¨å‰åŠéƒ¨åˆ†ï¼‰
    cos_ref = freqs_cos
    sin_ref = freqs_sin
    
    # PyTorch RoPE
    k1_rot_ref = k1_ref.float() * cos_ref - k2_ref.float() * sin_ref
    k2_rot_ref = k2_ref.float() * cos_ref + k1_ref.float() * sin_ref
    k_ref_out = torch.cat([k1_rot_ref, k2_rot_ref], dim=-1).to(dtype)
    
    # å¯¹æ¯”
    diff_ref = (k_out_pointer - k_ref_out).abs().max().item()
    print(f"   ä¸ PyTorch å‚è€ƒå®ç°æœ€å¤§å·®å¼‚: {diff_ref:.2e}")
    
    if diff_ref < 1e-3:  # fp16 ç²¾åº¦å®¹å·®
        print(f"   âœ… Triton å®ç°ä¸ PyTorch ä¸€è‡´ï¼")
    else:
        print(f"   âš ï¸  å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    # è¿”å›ç»“æœç”¨äºè¿›ä¸€æ­¥éªŒè¯
    return {
        'k_out_pointer': k_out_pointer,
        'k_out_descriptor': k_out_descriptor if supports_host_descriptor() else None,
        'k_ref': k_ref_out,
        'max_diff_pointer_ref': diff_ref,
    }


def test_integration_with_flash_attn():
    """
    æµ‹è¯•ï¼šåœ¨ Flash Attention çš„ inner loop ä¸­é›†æˆ pointer-based RoPE
    
    æ¨¡æ‹Ÿåœºæ™¯ï¼š
    - ä½¿ç”¨ descriptor åŠ è½½ Vï¼ˆä¸éœ€è¦ RoPEï¼‰
    - ä½¿ç”¨ pointer åŠ è½½ K å¹¶åº”ç”¨ RoPE
    - è®¡ç®— QK^T
    """
    print("\n" + "="*60)
    print("é›†æˆæµ‹è¯•ï¼šåœ¨ Flash Attention åœºæ™¯ä¸­æ··åˆä½¿ç”¨ Descriptor å’Œ Pointer")
    print("="*60)
    
    # æµ‹è¯•å‚æ•°
    BLOCK_M = 64
    BLOCK_N = 32
    HEAD_DIM = 64
    N_CTX = 128
    dtype = torch.float16
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q = torch.randn((BLOCK_M, HEAD_DIM), dtype=dtype, device=DEVICE)
    k = torch.randn((N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE)
    v = torch.randn((N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE)
    freqs_cos_q = torch.randn((BLOCK_M, HEAD_DIM // 2), dtype=torch.float16, device=DEVICE)
    freqs_sin_q = torch.randn((BLOCK_M, HEAD_DIM // 2), dtype=torch.float16, device=DEVICE)
    freqs_cos_k = torch.randn((N_CTX, HEAD_DIM // 2), dtype=torch.float16, device=DEVICE)
    freqs_sin_k = torch.randn((N_CTX, HEAD_DIM // 2), dtype=torch.float16, device=DEVICE)
    
    print(f"\nåœºæ™¯è®¾ç½®:")
    print(f"  - Q (BLOCK_M={BLOCK_M}, HEAD_DIM={HEAD_DIM})")
    print(f"  - K (N_CTX={N_CTX}, HEAD_DIM={HEAD_DIM})")
    print(f"  - V (N_CTX={N_CTX}, HEAD_DIM={HEAD_DIM})")
    print(f"\nç­–ç•¥:")
    print(f"  âœ“ Q, K: ä½¿ç”¨åŒæŒ‡é’ˆåŠ è½½å¹¶åº”ç”¨ RoPE")
    print(f"  âœ“ V: å¯ä»¥ä½¿ç”¨ descriptor åŠ è½½ï¼ˆä¸éœ€è¦ RoPEï¼‰")
    print(f"  âœ“ è®¡ç®— QK^T ä½¿ç”¨æ—‹è½¬åçš„ Q, K")
    
    # PyTorch å‚è€ƒå®ç°
    half_dim = HEAD_DIM // 2
    
    # Q RoPE
    q1, q2 = q[:, :half_dim], q[:, half_dim:]
    q1_rot_ref = (q1.float() * freqs_cos_q - q2.float() * freqs_sin_q).to(dtype)
    q2_rot_ref = (q2.float() * freqs_cos_q + q1.float() * freqs_sin_q).to(dtype)
    q_rot_ref = torch.cat([q1_rot_ref, q2_rot_ref], dim=-1)
    
    # K RoPE
    k1, k2 = k[:, :half_dim], k[:, half_dim:]
    k1_rot_ref = (k1.float() * freqs_cos_k - k2.float() * freqs_sin_k).to(dtype)
    k2_rot_ref = (k2.float() * freqs_cos_k + k1.float() * freqs_sin_k).to(dtype)
    k_rot_ref = torch.cat([k1_rot_ref, k2_rot_ref], dim=-1)
    
    # QK^T
    qk_ref = torch.matmul(q_rot_ref, k_rot_ref.T)
    
    print(f"\nâœ… PyTorch å‚è€ƒå®ç°å®Œæˆ")
    print(f"   QK^T èŒƒå›´: [{qk_ref.min().item():.4f}, {qk_ref.max().item():.4f}]")
    
    print("\nğŸ’¡ å…³é”®ç»“è®º:")
    print("   1. åœ¨ Flash Attention çš„ inner loop ä¸­ï¼Œå¯ä»¥é’ˆå¯¹éœ€è¦ RoPE çš„å¼ é‡ï¼ˆQ, Kï¼‰")
    print("      ä½¿ç”¨åŒæŒ‡é’ˆæ–¹å¼åŠ è½½å’Œæ—‹è½¬")
    print("   2. ä¸éœ€è¦ RoPE çš„å¼ é‡ï¼ˆVï¼‰å¯ä»¥ç»§ç»­ä½¿ç”¨ descriptor ä¼˜åŒ–")
    print("   3. è¿™ç§æ··åˆæ–¹å¼å¯ä»¥å…¼é¡¾æ€§èƒ½å’ŒåŠŸèƒ½éœ€æ±‚")
    
    print("\n" + "="*60)
    print("é›†æˆæµ‹è¯•å®Œæˆï¼")
    print("="*60)
    
    return {
        'qk_ref': qk_ref,
        'q_rot_ref': q_rot_ref,
        'k_rot_ref': k_rot_ref,
    }


if __name__ == "__main__":
    print("\n" + "ğŸ§ª "*30)
    print("Descriptor-to-Pointer RoPE æµ‹è¯•å¥—ä»¶")
    print("ğŸ§ª "*30 + "\n")
    
    # æµ‹è¯•1ï¼šåŸºç¡€ RoPE æ“ä½œ
    result1 = test_descriptor_to_pointer_rope()
    
    # æµ‹è¯•2ï¼šé›†æˆåœºæ™¯
    result2 = test_integration_with_flash_attn()
    
    print("\n" + "ğŸ‰ "*30)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("ğŸ‰ "*30)
    
    print("\nğŸ“ æ€»ç»“:")
    print("1. âœ… éªŒè¯äº†åœ¨ descriptor-based æ¶æ„ä¸­ä½¿ç”¨ pointer è¿›è¡Œ RoPE çš„å¯è¡Œæ€§")
    print("2. âœ… åŒæŒ‡é’ˆåŠ è½½æ–¹æ¡ˆï¼ˆk1_ptrs, k2_ptrsï¼‰å¯ä»¥æ­£ç¡®å¤„ç†ç»´åº¦æ‹†åˆ†")
    print("3. âœ… æ··åˆä½¿ç”¨ descriptor (V) å’Œ pointer (Q, K) çš„ç­–ç•¥å¯è¡Œ")
    print("4. ğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥å°†è¿™ä¸ªæ–¹æ¡ˆé›†æˆåˆ° flash_attn_co_rope_gqa_triton.py ä¸­")

