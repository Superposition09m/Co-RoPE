"""
GQA vs åŸå§‹ V2 æ€§èƒ½å¯¹æ¯”
å¯¹æ¯”åœ¨ç›¸åŒ query heads æ•°é‡ä¸‹ï¼ŒGQAï¼ˆæ›´å°‘çš„ KV headsï¼‰ç›¸æ¯”æ ‡å‡† MHA çš„æ€§èƒ½æå‡
"""

import torch
import triton
import sys
import os

sys.path.insert(0, os.path.dirname(__file__))

from flash_attn_v2_triton import attention as attention_v2
from flash_attn_co_rope_gqa_triton import attention as attention_gqa


def benchmark_config(B, H_Q, H_KV, N, D, dtype=torch.float16, num_repeats=100):
    """
    å¯¹æ¯”å•ä¸ªé…ç½®ä¸‹çš„æ€§èƒ½
    
    Args:
        B: batch size
        H_Q: query heads æ•°é‡
        H_KV: key/value heads æ•°é‡ï¼ˆGQAï¼‰
        N: sequence length
        D: head dimension
        dtype: æ•°æ®ç±»å‹
        num_repeats: é‡å¤æ¬¡æ•°
    """
    device = 'cuda'
    causal = True
    sm_scale = 1.0 / (D ** 0.5)
    warp_specialize = False
    
    # å‡†å¤‡è¾“å…¥
    torch.manual_seed(42)
    
    # V2 ç‰ˆæœ¬ï¼šæ‰€æœ‰ heads éƒ½æ˜¯ç‹¬ç«‹çš„ (H_Q == H_KV)
    q_v2 = torch.randn(B, H_Q, N, D, device=device, dtype=dtype)
    k_v2 = torch.randn(B, H_Q, N, D, device=device, dtype=dtype)
    v_v2 = torch.randn(B, H_Q, N, D, device=device, dtype=dtype)
    
    # GQA ç‰ˆæœ¬ï¼šæ›´å°‘çš„ KV heads
    q_gqa = torch.randn(B, H_Q, N, D, device=device, dtype=dtype)
    k_gqa = torch.randn(B, H_KV, N, D, device=device, dtype=dtype)
    v_gqa = torch.randn(B, H_KV, N, D, device=device, dtype=dtype)
    
    # Warmup
    for _ in range(5):
        _ = attention_v2(q_v2, k_v2, v_v2, causal, sm_scale, warp_specialize)
        _ = attention_gqa(q_gqa, k_gqa, v_gqa, causal, sm_scale, warp_specialize)
        torch.cuda.synchronize()
    
    # æµ‹é€Ÿ V2
    fn_v2 = lambda: attention_v2(q_v2, k_v2, v_v2, causal, sm_scale, warp_specialize)
    time_v2 = triton.testing.do_bench(fn_v2, rep=num_repeats)
    
    # æµ‹é€Ÿ GQA
    fn_gqa = lambda: attention_gqa(q_gqa, k_gqa, v_gqa, causal, sm_scale, warp_specialize)
    time_gqa = triton.testing.do_bench(fn_gqa, rep=num_repeats)
    
    # è®¡ç®—å†…å­˜å ç”¨ï¼ˆç†è®ºå€¼ï¼‰
    # Q: [B, H_Q, N, D]
    # V2: K,V éƒ½æ˜¯ [B, H_Q, N, D]
    # GQA: K,V éƒ½æ˜¯ [B, H_KV, N, D]
    bytes_per_element = 2 if dtype == torch.float16 else 4
    kv_memory_v2 = 2 * B * H_Q * N * D * bytes_per_element / (1024**2)  # MB
    kv_memory_gqa = 2 * B * H_KV * N * D * bytes_per_element / (1024**2)  # MB
    
    return {
        'time_v2': time_v2,
        'time_gqa': time_gqa,
        'speedup': time_v2 / time_gqa if time_gqa > 0 else 0,
        'kv_memory_v2': kv_memory_v2,
        'kv_memory_gqa': kv_memory_gqa,
        'memory_saved': kv_memory_v2 - kv_memory_gqa,
        'memory_ratio': kv_memory_v2 / kv_memory_gqa if kv_memory_gqa > 0 else 0,
    }


def main():
    print("=" * 80)
    print("GQA vs åŸå§‹ V2 æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    print("\nè¯´æ˜ï¼š")
    print("  - V2:  æ ‡å‡† Multi-Head Attention (H_Q == H_KV)")
    print("  - GQA: Grouped-Query Attention (H_Q > H_KV, KV heads è¢«å¤šä¸ª Q heads å…±äº«)")
    print("  - æµ‹è¯•é…ç½®: dtype=FP16, causal=True")
    print("=" * 80)
    
    # æµ‹è¯•é…ç½®åˆ—è¡¨
    configs = [
        # (B, H_Q, H_KV, N, D, æè¿°)
        (1, 8, 2, 128, 64, "å°åºåˆ— (N=128)"),
        (1, 8, 2, 512, 64, "ä¸­åºåˆ— (N=512)"),
        (1, 8, 2, 1024, 64, "é•¿åºåˆ— (N=1024)"),
        (1, 8, 2, 2048, 64, "é•¿åºåˆ— (N=2048)"),
        (2, 8, 2, 1024, 64, "Batch=2, N=1024"),
        (1, 32, 8, 1024, 64, "å¤§æ¨¡å‹é…ç½® (H=32)"),
        (1, 8, 1, 1024, 64, "MQA (H_KV=1)"),
    ]
    
    print()
    results = []
    
    for B, H_Q, H_KV, N, D, desc in configs:
        GROUP_SIZE = H_Q // H_KV
        print(f"\n{'='*80}")
        print(f"é…ç½®: {desc}")
        print(f"  B={B}, H_Q={H_Q}, H_KV={H_KV}, N={N}, D={D}, GROUP_SIZE={GROUP_SIZE}")
        print(f"{'='*80}")
        
        try:
            result = benchmark_config(B, H_Q, H_KV, N, D)
            results.append((desc, result))
            
            print(f"\næ€§èƒ½:")
            print(f"  V2 (H={H_Q}):        {result['time_v2']:.3f} ms")
            print(f"  GQA (H_Q={H_Q}, H_KV={H_KV}): {result['time_gqa']:.3f} ms")
            print(f"  âš¡ Speedup:          {result['speedup']:.2f}x")
            
            print(f"\nKV Cache å†…å­˜:")
            print(f"  V2:                 {result['kv_memory_v2']:.2f} MB")
            print(f"  GQA:                {result['kv_memory_gqa']:.2f} MB")
            print(f"  ğŸ’¾ Memory Saved:    {result['memory_saved']:.2f} MB ({(1 - 1/result['memory_ratio'])*100:.1f}% å‡å°‘)")
            
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ±‡æ€»è¡¨æ ¼
    print("\n" + "=" * 80)
    print("æ±‡æ€»è¡¨æ ¼")
    print("=" * 80)
    print(f"{'é…ç½®':<25} {'V2 (ms)':<12} {'GQA (ms)':<12} {'Speedup':<10} {'å†…å­˜èŠ‚çœ':<12}")
    print("-" * 80)
    for desc, result in results:
        speedup_color = "ğŸš€" if result['speedup'] > 1.2 else "  "
        memory_pct = (1 - 1/result['memory_ratio'])*100
        print(f"{desc:<25} {result['time_v2']:>8.3f}     {result['time_gqa']:>8.3f}     "
              f"{speedup_color}{result['speedup']:>5.2f}x    {memory_pct:>5.1f}%")
    
    print("\n" + "=" * 80)
    print("æ€»ç»“ï¼š")
    print("  âœ… GQA åœ¨ä¿æŒç›¸åŒ query heads çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å‡å°‘ KV heads å®ç°ï¼š")
    print("     - è®¡ç®—é€Ÿåº¦æå‡")
    print("     - KV cache å†…å­˜æ˜¾è‘—å‡å°‘ï¼ˆé‡è¦ï¼åœ¨æ¨ç†æ—¶å¯ä»¥æ”¯æŒæ›´å¤§çš„ batch sizeï¼‰")
    print("=" * 80)


if __name__ == "__main__":
    main()

