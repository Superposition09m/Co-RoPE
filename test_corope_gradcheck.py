"""
CoRoPE æ¢¯åº¦æ­£ç¡®æ€§æµ‹è¯•

éªŒè¯æ‰‹åŠ¨å®ç°çš„ backward å’Œ PyTorch autograd çš„ç»“æœä¸€è‡´
"""

import torch
import sys
from corope_attn_gqa_pytorch import attention_pytorch


def numerical_gradient(func, inputs, eps=1e-4):
    """
    ä½¿ç”¨æœ‰é™å·®åˆ†æ³•è®¡ç®—æ•°å€¼æ¢¯åº¦
    
    Args:
        func: è¾“å‡ºæ ‡é‡çš„å‡½æ•°
        inputs: è¾“å…¥å¼ é‡åˆ—è¡¨
        eps: æ‰°åŠ¨å¤§å°
    
    Returns:
        gradients: æ•°å€¼æ¢¯åº¦åˆ—è¡¨
    """
    gradients = []
    
    for input_tensor in inputs:
        if not input_tensor.requires_grad:
            gradients.append(None)
            continue
        
        grad = torch.zeros_like(input_tensor)
        
        # éå†æ¯ä¸ªå…ƒç´ 
        it = torch.nditer(input_tensor.cpu().numpy(), flags=['multi_index'])
        while not it.finished:
            idx = it.multi_index
            
            # f(x + eps)
            input_tensor.data[idx] += eps
            loss_plus = func()
            
            # f(x - eps)
            input_tensor.data[idx] -= 2 * eps
            loss_minus = func()
            
            # f(x) æ¢å¤
            input_tensor.data[idx] += eps
            
            # ä¸­å¿ƒå·®åˆ†ï¼š(f(x+eps) - f(x-eps)) / (2*eps)
            grad[idx] = (loss_plus - loss_minus) / (2 * eps)
            
            it.iternext()
        
        gradients.append(grad)
    
    return gradients


def test_corop_gradient_simple(B=1, H_q=4, H_kv=2, N=32, D=64, causal=True):
    """
    ç®€åŒ–çš„æ¢¯åº¦æµ‹è¯•ï¼šä½¿ç”¨ torch.autograd.gradcheck
    """
    print('='*80)
    print(f'CoRoPE æ¢¯åº¦æµ‹è¯•ï¼ˆtorch.autograd.gradcheckï¼‰')
    print('='*80)
    print(f'é…ç½®: B={B}, H_q={H_q}, H_kv={H_kv}, N={N}, D={D}, causal={causal}')
    print(f'GQA group_size: {H_q // H_kv}')
    
    device = 'cuda'
    theta = 10000.0
    sm_scale = 1.0 / (D ** 0.5)
    
    # åˆ›å»ºå°è§„æ¨¡è¾“å…¥ï¼ˆgradcheck å¯¹å¤§å¼ é‡å¾ˆæ…¢ï¼‰
    q = torch.randn(B, H_q, N, D, device=device, dtype=torch.float64, requires_grad=True)
    k = torch.randn(B, H_kv, N, D, device=device, dtype=torch.float64, requires_grad=True)
    v = torch.randn(B, H_kv, N, D, device=device, dtype=torch.float64, requires_grad=True)
    
    # å®šä¹‰æµ‹è¯•å‡½æ•°
    def func(q_in, k_in, v_in):
        return attention_pytorch(q_in, k_in, v_in, causal, sm_scale, theta)
    
    print('\næ‰§è¡Œ gradcheckï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...ï¼‰')
    
    try:
        # PyTorch çš„ gradcheck ä½¿ç”¨æœ‰é™å·®åˆ†éªŒè¯æ¢¯åº¦
        # eps: æ‰°åŠ¨å¤§å°
        # atol: ç»å¯¹è¯¯å·®å®¹å¿åº¦
        result = torch.autograd.gradcheck(
            func,
            (q, k, v),
            eps=1e-4,
            atol=1e-3,
            rtol=1e-2,
            raise_exception=False,
        )
        
        if result:
            print('âœ… gradcheck PASSED: æ‰‹åŠ¨æ¢¯åº¦ä¸æ•°å€¼æ¢¯åº¦ä¸€è‡´ï¼')
            return True
        else:
            print('âŒ gradcheck FAILED: æ¢¯åº¦ä¸åŒ¹é…')
            return False
            
    except Exception as e:
        print(f'âŒ gradcheck å‡ºé”™: {e}')
        import traceback
        traceback.print_exc()
        return False


def test_corope_gradient_manual(B=2, H_q=4, H_kv=2, N=64, D=64, causal=True):
    """
    æ‰‹åŠ¨æ¢¯åº¦å¯¹æ¯”æµ‹è¯•ï¼šå¯¹æ¯”è‡ªåŠ¨æ±‚å¯¼å’Œæ‰‹åŠ¨å®ç°
    """
    print('\n' + '='*80)
    print('CoRoPE æ¢¯åº¦æ‰‹åŠ¨éªŒè¯ï¼ˆå¯¹æ¯” autogradï¼‰')
    print('='*80)
    print(f'é…ç½®: B={B}, H_q={H_q}, H_kv={H_kv}, N={N}, D={D}, causal={causal}')
    print(f'GQA group_size: {H_q // H_kv}')
    
    device = 'cuda'
    dtype = torch.float32
    theta = 10000.0
    sm_scale = 1.0 / (D ** 0.5)
    
    # ========== æ–¹æ³• 1: ä½¿ç”¨æˆ‘ä»¬çš„æ‰‹åŠ¨ backward ==========
    print('\n[æ–¹æ³• 1: æ‰‹åŠ¨ Backward]')
    q1 = torch.randn(B, H_q, N, D, device=device, dtype=dtype, requires_grad=True)
    k1 = torch.randn(B, H_kv, N, D, device=device, dtype=dtype, requires_grad=True)
    v1 = torch.randn(B, H_kv, N, D, device=device, dtype=dtype, requires_grad=True)
    
    output1 = attention_pytorch(q1, k1, v1, causal, sm_scale, theta)
    
    # éšæœºæ¢¯åº¦
    grad_out = torch.randn_like(output1)
    output1.backward(grad_out)
    
    dq1 = q1.grad.clone()
    dk1 = k1.grad.clone()
    dv1 = v1.grad.clone()
    
    print(f'  dQ: mean={dq1.mean().item():.6e}, std={dq1.std().item():.6e}')
    print(f'  dK: mean={dk1.mean().item():.6e}, std={dk1.std().item():.6e}')
    print(f'  dV: mean={dv1.mean().item():.6e}, std={dv1.std().item():.6e}')
    
    # ========== æ–¹æ³• 2: ä½¿ç”¨ PyTorch autogradï¼ˆä¸è°ƒç”¨æ‰‹åŠ¨ backwardï¼‰==========
    print('\n[æ–¹æ³• 2: çº¯ Autogradï¼ˆä½œä¸ºå‚è€ƒï¼‰]')
    
    # é‡æ–°å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œè®© PyTorch è‡ªåŠ¨æ±‚å¯¼
    q2 = q1.detach().clone().requires_grad_(True)
    k2 = k1.detach().clone().requires_grad_(True)
    v2 = v1.detach().clone().requires_grad_(True)
    
    # ä½¿ç”¨ç›¸åŒçš„ forward é€»è¾‘ï¼Œä½†è®© PyTorch è‡ªåŠ¨æ±‚å¯¼
    # å¤åˆ¶ forward çš„æ ¸å¿ƒè®¡ç®—ï¼ˆä¸ä½¿ç”¨è‡ªå®šä¹‰ Functionï¼‰
    with torch.enable_grad():
        # GQA expansion
        if H_q == H_kv:
            k_exp2 = k2
            v_exp2 = v2
        else:
            group_size = H_q // H_kv
            k_exp2 = k2.view(B, H_kv, 1, N, D).expand(B, H_kv, group_size, N, D).reshape(B, H_q, N, D)
            v_exp2 = v2.view(B, H_kv, 1, N, D).expand(B, H_kv, group_size, N, D).reshape(B, H_q, N, D)
        
        # è®¡ç®—æ­¥é•¿èƒ½é‡
        z_scores2 = torch.einsum('bhqd,bhkd->bhqk', q2, k_exp2) * sm_scale
        z2 = torch.sigmoid(z_scores2)
        
        # è®¡ç®—ç´¯ç§¯é‡Œç¨‹
        a_q2 = torch.cumsum(z2, dim=-1)
        a_q_total2 = torch.diagonal(a_q2, dim1=-2, dim2=-1)
        z_avg2 = z2.mean(dim=2, keepdim=True)
        a_k2 = torch.cumsum(z_avg2.squeeze(2), dim=-1)
        
        # é‡Œç¨‹å·®
        mileage_diff2 = a_k2.unsqueeze(2) - a_q_total2.unsqueeze(3)
        
        # é¢‘ç‡
        inv_freq2 = 1.0 / (theta ** (torch.arange(0, D, 2, device=device).float() / D))
        
        # æ—‹è½¬è§’åº¦
        angles2 = mileage_diff2.unsqueeze(-1) * inv_freq2
        cos_m2 = torch.cos(angles2)
        sin_m2 = torch.sin(angles2)
        
        # Split layout
        half_dim = D // 2
        q1_2, q2_2 = q2[..., :half_dim], q2[..., half_dim:]
        k1_2, k2_2 = k_exp2[..., :half_dim], k_exp2[..., half_dim:]
        
        # æ‰©å±•ç»´åº¦
        q1_e = q1_2.unsqueeze(3)
        q2_e = q2_2.unsqueeze(3)
        k1_e = k1_2.unsqueeze(2)
        k2_e = k2_2.unsqueeze(2)
        
        # æ—‹è½¬ç‚¹ç§¯
        real2 = q1_e * k1_e + q2_e * k2_e
        imag2 = q2_e * k1_e - q1_e * k2_e
        rotated2 = real2 * cos_m2 - imag2 * sin_m2
        attn_scores2 = rotated2.sum(dim=-1)
        
        # Causal mask
        if causal:
            mask2 = torch.triu(torch.ones(N, N, device=device, dtype=torch.bool), diagonal=1)
            attn_scores2 = attn_scores2.masked_fill(mask2.unsqueeze(0).unsqueeze(0), float('-inf'))
        
        # Softmax
        attn_weights2 = torch.nn.functional.softmax(attn_scores2, dim=-1, dtype=torch.float32).to(dtype)
        
        # Output
        output2 = torch.einsum('bhqk,bhkd->bhqd', attn_weights2, v_exp2)
    
    # Backward
    output2.backward(grad_out)
    
    dq2 = q2.grad.clone()
    dk2 = k2.grad.clone()
    dv2 = v2.grad.clone()
    
    print(f'  dQ: mean={dq2.mean().item():.6e}, std={dq2.std().item():.6e}')
    print(f'  dK: mean={dk2.mean().item():.6e}, std={dk2.std().item():.6e}')
    print(f'  dV: mean={dv2.mean().item():.6e}, std={dv2.std().item():.6e}')
    
    # ========== å¯¹æ¯”æ¢¯åº¦ ==========
    print('\n' + '='*80)
    print('æ¢¯åº¦å¯¹æ¯”')
    print('='*80)
    
    def compare_gradients(g1, g2, name):
        abs_diff = torch.abs(g1 - g2)
        rel_diff = abs_diff / (torch.abs(g2) + 1e-8)
        
        print(f'\n{name}:')
        print(f'  Max Abs Error:  {abs_diff.max().item():.6e}')
        print(f'  Mean Abs Error: {abs_diff.mean().item():.6e}')
        print(f'  Max Rel Error:  {rel_diff.max().item():.6e}')
        print(f'  Mean Rel Error: {rel_diff.mean().item():.6e}')
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡
        passed = abs_diff.max().item() < 1e-3 and rel_diff.max().item() < 0.1
        if passed:
            print(f'  âœ… {name} æ¢¯åº¦ä¸€è‡´')
        else:
            print(f'  âŒ {name} æ¢¯åº¦ä¸åŒ¹é…')
        
        return passed
    
    dq_pass = compare_gradients(dq1, dq2, 'dQ')
    dk_pass = compare_gradients(dk1, dk2, 'dK')
    dv_pass = compare_gradients(dv1, dv2, 'dV')
    
    all_passed = dq_pass and dk_pass and dv_pass
    
    print('\n' + '='*80)
    if all_passed:
        print('ğŸ‰ æ‰€æœ‰æ¢¯åº¦æµ‹è¯•é€šè¿‡ï¼æ‰‹åŠ¨ backward å®ç°æ­£ç¡®ï¼')
    else:
        print('âŒ éƒ¨åˆ†æ¢¯åº¦æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ backward å®ç°')
    print('='*80)
    
    return all_passed


def test_corope_output_consistency():
    """
    æµ‹è¯•ä¸¤ç§æ–¹å¼çš„è¾“å‡ºæ˜¯å¦ä¸€è‡´ï¼ˆä½œä¸ºå‰ç½®æ£€æŸ¥ï¼‰
    """
    print('\n' + '='*80)
    print('å‰ç½®æ£€æŸ¥ï¼šForward è¾“å‡ºä¸€è‡´æ€§')
    print('='*80)
    
    B, H_q, H_kv, N, D = 2, 4, 2, 64, 64
    device = 'cuda'
    dtype = torch.float32
    theta = 10000.0
    sm_scale = 1.0 / (D ** 0.5)
    
    torch.manual_seed(42)
    q = torch.randn(B, H_q, N, D, device=device, dtype=dtype)
    k = torch.randn(B, H_kv, N, D, device=device, dtype=dtype)
    v = torch.randn(B, H_kv, N, D, device=device, dtype=dtype)
    
    # æ–¹æ³•1ï¼šä½¿ç”¨è‡ªå®šä¹‰ Function
    output1 = attention_pytorch(q, k, v, True, sm_scale, theta)
    
    # æ–¹æ³•2ï¼šåº”è¯¥å¾—åˆ°ç›¸åŒç»“æœï¼ˆå› ä¸ºç”¨çš„æ˜¯åŒä¸€å¥— forward é€»è¾‘ï¼‰
    q2 = q.clone()
    k2 = k.clone()
    v2 = v.clone()
    output2 = attention_pytorch(q2, k2, v2, True, sm_scale, theta)
    
    diff = torch.abs(output1 - output2)
    print(f'  Output max diff: {diff.max().item():.6e}')
    print(f'  Output mean diff: {diff.mean().item():.6e}')
    
    if diff.max().item() < 1e-6:
        print('  âœ… Forward è¾“å‡ºä¸€è‡´')
        return True
    else:
        print('  âŒ Forward è¾“å‡ºä¸ä¸€è‡´ï¼ˆè¿™ä¸åº”è¯¥å‘ç”Ÿï¼‰')
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æ¢¯åº¦æµ‹è¯•"""
    print('='*80)
    print('CoRoPE GQA æ¢¯åº¦æ­£ç¡®æ€§æµ‹è¯•å¥—ä»¶')
    print('='*80)
    
    # å‰ç½®æ£€æŸ¥
    if not test_corope_output_consistency():
        print('\nâŒ Forward è¾“å‡ºä¸ä¸€è‡´ï¼Œåœæ­¢æµ‹è¯•')
        sys.exit(1)
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        # (B, H_q, H_kv, N, D, causal, name)
        (1, 4, 4, 32, 64, True, 'MHA-Small'),
        (2, 4, 2, 32, 64, True, 'GQA-Small'),
        (1, 8, 2, 64, 64, True, 'GQA-Medium'),
        (1, 4, 4, 32, 64, False, 'MHA-No-Causal'),
    ]
    
    all_passed = True
    
    for B, H_q, H_kv, N, D, causal, name in configs:
        print(f'\n{"#"*80}')
        print(f'æµ‹è¯•é…ç½®: {name}')
        print(f'{"#"*80}')
        
        try:
            passed = test_corope_gradient_manual(B, H_q, H_kv, N, D, causal)
            if not passed:
                all_passed = False
        except Exception as e:
            print(f'âŒ é…ç½® {name} æµ‹è¯•å¤±è´¥: {e}')
            import traceback
            traceback.print_exc()
            all_passed = False
    
    # æœ€ç»ˆæ€»ç»“
    print('\n' + '='*80)
    print('æœ€ç»ˆç»“æœ')
    print('='*80)
    
    if all_passed:
        print('ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰æ¢¯åº¦æµ‹è¯•é€šè¿‡ï¼')
        print('CoRoPE çš„æ‰‹åŠ¨ backward å®ç°å®Œå…¨æ­£ç¡®ï¼')
        return 0
    else:
        print('âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤ backward å®ç°')
        return 1


if __name__ == '__main__':
    sys.exit(main())

