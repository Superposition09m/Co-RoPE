# Fused RoPE é•¿åºåˆ—æ€§èƒ½ä¼˜åŒ–åˆ†æ

## ğŸ”´ å½“å‰é—®é¢˜

æ ¹æ® benchmark ç»“æœï¼ŒFused RoPE åœ¨é•¿åºåˆ—æ—¶æ€§èƒ½ä¸¥é‡é€€åŒ–ï¼š

| åºåˆ—é•¿åº¦ | Forward Speedup vs B3 | Backward Speedup vs B3 | çŠ¶æ€ |
|---------|----------------------|------------------------|------|
| 512-4K  | **1.2-4.9x â†‘** | **1.1-2.1x â†‘** | âœ… ä¼˜ç§€ |
| 8K-16K  | **0.8-0.9x â†“** | **0.7-0.8x â†“** | âš ï¸ é€€åŒ– |
| 32K-128K | **0.47-0.57x â†“** | **0.8x â†“** | âŒ ä¸¥é‡é€€åŒ– |

**ç»“è®º**ï¼šåœ¨ â‰¥8K åºåˆ—é•¿åº¦æ—¶ï¼ŒFused RoPE åè€Œæ¯”éèåˆç‰ˆæœ¬æ…¢ã€‚

---

## ğŸ” æ ¹æœ¬åŸå› 

### 1. **Register Pressureï¼ˆå¯„å­˜å™¨å‹åŠ›ï¼‰**

Fused RoPE kernel éœ€è¦å­˜å‚¨ï¼š
```python
# Baseline 3 (æ— RoPEèåˆ):
q, k1_rot, k2_rot, v  # åŠ è½½åçš„æ•°æ®

# Fused RoPE (æœ‰RoPEèåˆ):
q1, q2, q1_rot, q2_rot  # Q çš„åŸå§‹å’Œæ—‹è½¬ç‰ˆæœ¬
k1, k2, k1_rot, k2_rot  # K çš„åŸå§‹å’Œæ—‹è½¬ç‰ˆæœ¬
cos_k, sin_k            # é¢‘ç‡æ•°æ®
v                       # V
```

**å¢åŠ çš„å¯„å­˜å™¨ä½¿ç”¨ï¼š**
- é¢å¤–çš„ `q1, q2, k1, k2, cos, sin` å˜é‡
- ä¼°è®¡å¢åŠ  **20-40% å¯„å­˜å™¨**

**åæœï¼š**
- SM èƒ½åŒæ—¶è¿è¡Œçš„ warp æ•°é‡å‡å°‘ â†’ **Occupancy ä¸‹é™**
- æ— æ³•éšè—å†…å­˜å»¶è¿Ÿ â†’ **æ€§èƒ½ä¸‹é™**

### 2. **L2 Cache Miss**

é•¿åºåˆ—çš„æ•°æ®é‡ï¼š
```
32K seq: 4 * 1 * 32 * 32768 * 128 * 2B = 1024 MB > 60 MB L2
64K seq: 2048 MB >> 60 MB L2
```

å½“æ•°æ®æ— æ³•æ”¾å…¥ L2 cache æ—¶ï¼Œé¢‘ç¹è®¿é—® HBM å¯¼è‡´ bandwidth æˆä¸ºç“¶é¢ˆã€‚

### 3. **Tile é…ç½®ä¸é€‚åˆé•¿åºåˆ—**

å½“å‰ autotune é…ç½®ï¼š
```python
for BM in [64, 128]
for BN in [32, 64, 128]
```

é•¿åºåˆ—æ—¶ï¼Œåº”è¯¥ï¼š
- **å‡å° BLOCK_M/BLOCK_N** ä»¥é™ä½å¯„å­˜å™¨ä½¿ç”¨
- **å¢åŠ  pipeline stages** ä»¥æ›´å¥½åœ° overlap è®¡ç®—å’Œå†…å­˜è®¿é—®

---

## ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šåŠ¨æ€ Kernel Selectionï¼ˆæœ€ç®€å•ï¼‰

æ ¹æ®åºåˆ—é•¿åº¦é€‰æ‹©ä¸åŒå®ç°ï¼š

```python
def attention_adaptive(q, k, v, causal, sm_scale, freqs_cos, freqs_sin):
    seq_len = q.shape[2]
    
    if seq_len <= 4096:
        # çŸ­åºåˆ—ï¼šä½¿ç”¨ Fused RoPEï¼ˆæ€§èƒ½æœ€ä½³ï¼‰
        return fused_rope_attn(q, k, v, causal, sm_scale, freqs_cos, freqs_sin, False)
    else:
        # é•¿åºåˆ—ï¼šä½¿ç”¨éèåˆç‰ˆæœ¬ï¼ˆé¿å… register pressureï¼‰
        return baseline3_rope_flashattn_triton(q, k, v, causal, sm_scale)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•ç›´æ¥ï¼Œç«‹å³ç”Ÿæ•ˆ
- âœ… åœ¨æ‰€æœ‰åºåˆ—é•¿åº¦éƒ½è·å¾—æœ€ä¼˜æ€§èƒ½

**ç¼ºç‚¹**ï¼š
- âŒ ä¸æ˜¯"çœŸæ­£"çš„ä¼˜åŒ–ï¼Œåªæ˜¯ workaround
- âŒ éœ€è¦ç»´æŠ¤ä¸¤å¥—ä»£ç 

### æ–¹æ¡ˆ 2ï¼šä¼˜åŒ– Tile é…ç½®ï¼ˆæ¨èï¼‰

ä¸ºé•¿åºåˆ—æ·»åŠ ä¸“é—¨çš„ autotune é…ç½®ï¼š

```python
# åœ¨ fused_rope_attn.py ä¸­ä¿®æ”¹ configs
configs = [
    # åŸæœ‰é…ç½®ï¼ˆé€‚åˆçŸ­åºåˆ—ï¼‰
    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w)
    for BM in [64, 128]
    for BN in [32, 64, 128]
    for s in [2, 3, 4]
    for w in [4, 8]
]

# ä¸ºé•¿åºåˆ—æ·»åŠ å° tile é…ç½®
if N_CTX >= 8192:
    configs += [
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_stages=4, num_warps=4),
    ]
```

**å…³é”®æ€è·¯**ï¼š
- å° tile â†’ æ›´å°‘å¯„å­˜å™¨ â†’ æ›´é«˜ occupancy
- æ›´å¤š stages â†’ æ›´å¥½çš„ memory latency hiding

### æ–¹æ¡ˆ 3ï¼šå‡å°‘ä¸­é—´å˜é‡ï¼ˆä¾µå…¥å¼ï¼‰

åœ¨ `_attn_fwd_inner` kernel ä¸­ï¼š

```python
# å½“å‰å®ç°ï¼ˆä¿ç•™åŸå§‹ q1, q2, k1, k2ï¼‰:
q1 = tl.load(...)
q2 = tl.load(...)
q1_rot = (q1 * cos - q2 * sin).to(dtype)
q2_rot = (q2 * cos + q1 * sin).to(dtype)

# ä¼˜åŒ–ï¼šç«‹å³ overwriteï¼Œä¸ä¿ç•™åŸå§‹
q1 = tl.load(...)
q2 = tl.load(...)
q1_temp = q1  # ä¸´æ—¶ä¿å­˜ç”¨äºè®¡ç®— q2_rot
q1 = (q1 * cos - q2 * sin).to(dtype)  # q1 â†’ q1_rot
q2 = (q2 * cos + q1_temp * sin).to(dtype)  # q2 â†’ q2_rot
```

**ä¼˜ç‚¹**ï¼šå‡å°‘æ´»è·ƒå¯„å­˜å™¨æ•°é‡
**ç¼ºç‚¹**ï¼šä»£ç å¯è¯»æ€§ä¸‹é™

### æ–¹æ¡ˆ 4ï¼šä½¿ç”¨ Shared Memoryï¼ˆé«˜çº§ï¼‰

å°† `freqs_cos`, `freqs_sin` æ”¾å…¥ shared memory è€Œéå¯„å­˜å™¨ï¼š

```python
# åœ¨ kernel å¼€å§‹æ—¶åŠ è½½åˆ° SMEM
freqs_cos_smem = tl.shared_memory(...)
freqs_sin_smem = tl.shared_memory(...)
```

**ä¼˜ç‚¹**ï¼šå¤§å¹…å‡å°‘å¯„å­˜å™¨å‹åŠ›
**ç¼ºç‚¹**ï¼šå¢åŠ  SMEM ä½¿ç”¨ï¼Œå¯èƒ½å½±å“ occupancyï¼ˆä¸åŒçš„ç“¶é¢ˆï¼‰

---

## ğŸ¯ ç«‹å³å¯è¡Œçš„ä¼˜åŒ–ï¼ˆæ–¹æ¡ˆ 2 ç®€åŒ–ç‰ˆï¼‰

ä¿®æ”¹ `fused_rope_attn.py` çš„ `prune_invalid_configs` å‡½æ•°ï¼š

```python
def prune_invalid_configs(configs, named_args, **kwargs):
    N_CTX = kwargs["N_CTX"]
    STAGE = kwargs["STAGE"]
    
    filtered = [
        conf for conf in configs 
        if conf.kwargs.get("BLOCK_M", 0) <= N_CTX 
        and (conf.kwargs.get("BLOCK_M", 0) >= conf.kwargs.get("BLOCK_N", 0) or STAGE == 1)
    ]
    
    # é•¿åºåˆ—æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨å° tile ä»¥é™ä½å¯„å­˜å™¨å‹åŠ›
    if N_CTX >= 8192:
        # è¿‡æ»¤æ‰å¤§ tile (BLOCK_M >= 128 or BLOCK_N >= 128)
        filtered = [
            conf for conf in filtered
            if conf.kwargs.get("BLOCK_M", 0) <= 64 
            and conf.kwargs.get("BLOCK_N", 0) <= 64
        ]
    
    return filtered
```

---

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®æ”¹åï¼Œé•¿åºåˆ—æ€§èƒ½åº”æå‡åˆ°ï¼š
- 8K-16K: **0.9-1.0x**ï¼ˆæ¥è¿‘ Baseline 3ï¼‰
- 32K-128K: **0.8-0.9x**ï¼ˆæ˜¾è‘—æ”¹å–„ï¼Œè™½ç„¶å¯èƒ½ä»ç•¥æ…¢ï¼‰

å¦‚æœä»ä¸ç†æƒ³ï¼Œå»ºè®®é‡‡ç”¨**æ–¹æ¡ˆ 1**ï¼ˆåŠ¨æ€é€‰æ‹©ï¼‰ä½œä¸º production æ–¹æ¡ˆã€‚

---

## ğŸ› ï¸ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å°è¯•**ï¼šä¿®æ”¹ `prune_invalid_configs`ï¼ˆ10åˆ†é’Ÿï¼‰
2. **éªŒè¯æ•ˆæœ**ï¼šé‡æ–°è¿è¡Œ `bench_compare.py`ï¼ˆ5åˆ†é’Ÿï¼‰
3. **å¦‚æœä»ä¸ç†æƒ³**ï¼šå®ç°æ–¹æ¡ˆ 1 çš„ adaptive kernel selection
4. **é«˜çº§ä¼˜åŒ–**ï¼šä½¿ç”¨ `ncu` profile å®šä½å…·ä½“ç“¶é¢ˆ

éœ€è¦æˆ‘ç°åœ¨å°±å®ç°æ–¹æ¡ˆ 2 å—ï¼Ÿ

